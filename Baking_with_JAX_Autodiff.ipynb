{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mridul-sahu/baking-with-jax-autodiff/blob/main/Baking_with_JAX_Autodiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baking with JAX Autodiff üç∞"
      ],
      "metadata": {
        "id": "CZVVbZxuxuH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's bake a cake!\n",
        "Well, not literally, but we'll use the process of perfecting a cake recipe as our story to understand JAX's automatic differentiation (autodiff) capabilities, from the fundamentals to more advanced techniques.\n",
        "\n",
        "Imagine we are bakers, and JAX is our incredibly smart assistant. Our goal is to create the perfect cake. This involves adjusting ingredients (inputs) and observing the cake's properties (outputs) like sweetness, fluffiness, or overall deliciousness score. To do this efficiently, we need to know how tweaking each ingredient affects the final cake. This \"how things change\" is the essence of differentiation, and JAX's autodiff is our tool to calculate it automatically."
      ],
      "metadata": {
        "id": "ITEUq-JFxeyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our Baking Course\n",
        "\n",
        "1.  **The Basics**\n",
        "    Getting started with gradients (`jax.grad`), handling different input/output structures, and verifying results.\n",
        "2.  **Advanced Baking**\n",
        "    Dealing with multiple cake properties at once (Jacobians), understanding the rate at which improvements change (Hessians), efficiently calculating directional changes (JVPs, VJPs, HVPs), and controlling gradient flow (`stop_gradient`, `vmap`).\n",
        "3.  **Exotic Flavors & Vibrations**\n",
        "    Exploring how JAX differentiates functions involving complex numbers, understanding the difference between 'smooth' (holomorphic) and 'tricky' (non-holomorphic) cases, and using the right tools (`grad` with `holomorphic=True`, Jacobians) accordingly.\n",
        "4.  **Secret Family Recipes**\n",
        "    Teaching JAX custom differentiation rules using `jax.custom_jvp` and `jax.custom_vjp` to overcome limitations, such as fixing numerical instability, enforcing specific baking rules (like gradient clipping), or handling complex iterative processes (like dough maturation) that standard autodiff struggles with."
      ],
      "metadata": {
        "id": "aOxy1c02yQpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Core Concepts: The Building Blocks of Change\n",
        "\n",
        "Before diving into JAX functions, let's understand the ideas:\n",
        "\n",
        "1. **Derivative**: If we change *one* ingredient (e.g., sugar) slightly, how much does *one* property (e.g., sweetness) change? The derivative measures this instantaneous rate of change.\n",
        "\n",
        "2. **Gradient**: If we have *multiple* ingredients affecting *one* key outcome (e.g., an \"overall deliciousness score\"), the gradient is a list (vector) of derivatives. Each element tells us how the score changes with respect to one specific ingredient. The gradient points in the direction of ingredient adjustments that *most rapidly* increase the score.\n",
        "\n",
        "3. **Jacobian**: What if changing *multiple* ingredients (sugar, flour, eggs) affects *multiple* properties (sweetness, fluffiness, cost)? The Jacobian is a table (matrix) containing *all* the partial derivatives ‚Äì how *each* ingredient influences *each* property. It's the complete map of local sensitivities.\n",
        "\n",
        "4. **Hessian**: This describes the *curvature* of our outcome. If we're adjusting ingredients to improve the deliciousness score, are we approaching the peak rapidly (high curvature) or is the landscape flat (low curvature)? It's the matrix of *second* derivatives, telling us how the *gradient* itself changes.\n",
        "\n",
        "5. **Forward vs. Reverse Mode Autodiff**: Two main computational strategies:\n",
        "\n",
        "  - **Forward Mode (JVP)**: Tracks changes *forward*. Efficient when you have *fewer* inputs than outputs. Think: \"Let's nudge the sugar amount and see how it affects sweetness, fluffiness, and cost.\"\n",
        "\n",
        "  - **Reverse Mode (VJP)**: Works *backward* from the output. Efficient when you have *more* inputs than outputs (like many ingredients affecting one score). Think: \"To make the cake slightly sweeter, how should all the ingredients (sugar, flour, eggs...) have changed?\""
      ],
      "metadata": {
        "id": "D78i73mFy9S1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1: The Basics - Simple Adjustments & Verification\n",
        "\n",
        "Let's start with the fundamental tool for finding gradients.\n"
      ],
      "metadata": {
        "id": "er-HdHiJ029r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Taking Gradients with \"jax.grad\"\n",
        "\n",
        "JAX's primary function for differentiation is `jax.grad()`. It takes a Python function (that uses JAX-compatible operations) which returns a *single scalar value*, and `jax.grad()` returns a *new function* that computes the gradient of the original function.\n",
        "\n",
        "*Example:* A Simple Function (`tanh`)\n",
        "\n",
        "The `tanh` function is often used in neural networks. Let's find its derivative."
      ],
      "metadata": {
        "id": "xcdm4mnc09mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# The function we want to differentiate\n",
        "f_tanh = jnp.tanh\n",
        "\n",
        "# Use jax.grad to get a function that computes the gradient (derivative)\n",
        "grad_tanh = jax.grad(f_tanh)\n",
        "\n",
        "# Evaluate the gradient function at a specific point (e.g., x=2.0)\n",
        "gradient_value = grad_tanh(2.0)\n",
        "print(f\"The gradient of tanh at x=2.0 is: {gradient_value}\")\n",
        "\n",
        "# For tanh, the derivative is 1 - tanh(x)^2 or sech(x)^2\n",
        "# Let's check: 1 - jnp.tanh(2.0)**2 = 0.07065...\n",
        "assert jnp.allclose(gradient_value, 1 - jnp.tanh(2.0)**2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN9rYwaNwoNP",
        "outputId": "702598fe-b5d2-4ed5-9073-940056ab9ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The gradient of tanh at x=2.0 is: 0.07065081596374512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Higher-Order Derivatives*\n",
        "\n",
        "Since `jax.grad(f)` returns a function, we can apply `jax.grad` again to get second, third, or higher-order derivatives.\n",
        "\n",
        "*Example: A Polynomial Recipe*\n",
        "\n",
        "Imagine a simplified \"cake quality\" score based on one ingredient `x`: $f(x)=x^3 + 2x^2 ‚àí 3x+1$."
      ],
      "metadata": {
        "id": "2o4BwA4k3rGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cake_quality_simple(x):\n",
        "  \"\"\"A simple score based on one ingredient amount x.\"\"\"\n",
        "  return x**3 + 2*x**2 - 3*x + 1\n",
        "\n",
        "# First derivative function (how score changes with x)\n",
        "d_quality_dx = jax.grad(cake_quality_simple)\n",
        "\n",
        "# Second derivative function (how the *rate of change* changes)\n",
        "d2_quality_dx2 = jax.grad(d_quality_dx) # or grad(grad(cake_quality_simple))\n",
        "\n",
        "# Third derivative function\n",
        "d3_quality_dx3 = jax.grad(d2_quality_dx2)\n",
        "\n",
        "# Fourth derivative function\n",
        "d4_quality_dx4 = jax.grad(d3_quality_dx3)\n",
        "\n",
        "# Let's evaluate these at x = 1.0\n",
        "x_value = 1.0\n",
        "print(f\"\\n--- Higher-Order Derivatives at x={x_value} ---\")\n",
        "print(f\"f'(x)   = {d_quality_dx(x_value)}\")   # Expected: 3*(1)^2 + 4*(1) - 3 = 4\n",
        "print(f\"f''(x)  = {d2_quality_dx2(x_value)}\") # Expected: 6*(1) + 4 = 10\n",
        "print(f\"f'''(x) = {d3_quality_dx3(x_value)}\") # Expected: 6\n",
        "print(f\"f''''(x)= {d4_quality_dx4(x_value)}\") # Expected: 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m12Qu_K9wuI2",
        "outputId": "2bd75bc1-f792-4a10-ff28-dba7dc0a6643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Higher-Order Derivatives at x=1.0 ---\n",
            "f'(x)   = 4.0\n",
            "f''(x)  = 10.0\n",
            "f'''(x) = 6.0\n",
            "f''''(x)= 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Computing Gradients for a Recipe (Logistic Regression Example)\n",
        "\n",
        "Let's adapt the logistic regression example. Imagine we have features derived from our ingredients (`inputs`) and we want to predict if a customer will like the cake (`True`/`False`). We have parameters `W` (weights) and `b` (bias) for our prediction model. Our goal is to adjust `W` and `b` to minimize a `loss` function (like prediction error). We need the gradient of the `loss` with respect to `W` and `b`."
      ],
      "metadata": {
        "id": "wkYdiyss5yBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.key(42)\n",
        "\n",
        "def sigmoid(x):\n",
        "  \"\"\"Sigmoid activation function.\"\"\"\n",
        "  return 0.5 * (jnp.tanh(x / 2) + 1)\n",
        "\n",
        "def predict_like(W, b, ingredient_features):\n",
        "  \"\"\"Predicts probability of liking the cake.\"\"\"\n",
        "  # Linear model followed by sigmoid\n",
        "  logit = jnp.dot(ingredient_features, W) + b\n",
        "  return sigmoid(logit)\n",
        "\n",
        "# Toy data: [sweetness, fluffiness, cost_factor] features for 4 cakes\n",
        "ingredient_features = jnp.array([[0.8, 0.9, -0.5],\n",
        "                                 [0.7, 0.8, -0.4],\n",
        "                                 [0.3, 0.4, -0.8], # Less sweet/fluffy, low cost\n",
        "                                 [0.9, 0.7, -0.3]])\n",
        "# Did customers like these cakes? (True/False)\n",
        "customer_likes = jnp.array([True, True, False, True])\n",
        "\n",
        "def calculate_loss(W, b, ingredient_features, customer_likes):\n",
        "  \"\"\"Calculates the negative log-likelihood loss.\"\"\"\n",
        "  predictions = predict_like(W, b, ingredient_features)\n",
        "  # Formula for binary cross-entropy / negative log-likelihood\n",
        "  # Avoid log(0) with a small epsilon\n",
        "  epsilon = 1e-7\n",
        "  label_probabilities = predictions * customer_likes + (1 - predictions) * (1 - customer_likes)\n",
        "  return -jnp.sum(jnp.log(label_probabilities + epsilon))\n",
        "\n",
        "# Initialize random parameters for our prediction model\n",
        "key, W_key, b_key = random.split(key, 3)\n",
        "# W has shape (num_features,) = (3,)\n",
        "W = random.normal(W_key, (3,))\n",
        "b = random.normal(b_key, ()) # bias is a scalar\n",
        "\n",
        "# --- Calculate Gradients ---\n",
        "# Use argnums to specify which positional arguments to differentiate w.r.t.\n",
        "\n",
        "# Gradient w.r.t. W (the 0-th argument)\n",
        "grad_loss_W_fn = jax.grad(calculate_loss, argnums=0)\n",
        "W_gradient = grad_loss_W_fn(W, b, ingredient_features, customer_likes)\n",
        "print(f\"\\n--- Gradients for Cake Likelihood Model ---\")\n",
        "print(f\"Gradient w.r.t. W (argnums=0):\\n{W_gradient}\")\n",
        "\n",
        "# Gradient w.r.t. b (the 1st argument)\n",
        "grad_loss_b_fn = jax.grad(calculate_loss, argnums=1)\n",
        "b_gradient = grad_loss_b_fn(W, b, ingredient_features, customer_likes)\n",
        "print(f\"\\nGradient w.r.t. b (argnums=1):\\n{b_gradient}\")\n",
        "\n",
        "# Gradient w.r.t. both W and b (arguments 0 and 1)\n",
        "# Returns a tuple of gradients in the same order as argnums\n",
        "grad_loss_Wb_fn = jax.grad(calculate_loss, argnums=(0, 1))\n",
        "W_gradient_tuple, b_gradient_tuple = grad_loss_Wb_fn(W, b, ingredient_features, customer_likes)\n",
        "print(f\"\\nGradient w.r.t. W (from tuple):\\n{W_gradient_tuple}\")\n",
        "print(f\"Gradient w.r.t. b (from tuple):\\n{b_gradient_tuple}\")\n",
        "\n",
        "# Default is argnums=0\n",
        "assert jnp.allclose(jax.grad(calculate_loss)(W,b, ingredient_features, customer_likes), W_gradient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGB5BASs5JAP",
        "outputId": "a18628fc-1cf7-4bd4-99e3-b9e437c98280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Gradients for Cake Likelihood Model ---\n",
            "Gradient w.r.t. W (argnums=0):\n",
            "[-0.05690404  0.03078762 -0.52113414]\n",
            "\n",
            "Gradient w.r.t. b (argnums=1):\n",
            "0.45482203364372253\n",
            "\n",
            "Gradient w.r.t. W (from tuple):\n",
            "[-0.05690404  0.03078762 -0.52113414]\n",
            "Gradient w.r.t. b (from tuple):\n",
            "0.45482203364372253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These gradients tell us how to adjust W and b to reduce the prediction error (the loss).\n",
        "\n"
      ],
      "metadata": {
        "id": "A5VRa_9g8CaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Differentiating with Respect to a Model (PyTrees)\n",
        "\n",
        "Model parameters might are often grouped into dictionaries or lists (e.g., `params = {'W': W, 'b': b}`). JAX handles differentiation through these standard Python containers (called PyTrees) seamlessly."
      ],
      "metadata": {
        "id": "qQB6FdCi8I_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine parameters into a dictionary\n",
        "params = {'W': W, 'b': b}\n",
        "\n",
        "def calculate_loss_dict(parameters, ingredient_features, customer_likes):\n",
        "  \"\"\"Loss function taking parameters as a dictionary.\"\"\"\n",
        "  # Access parameters by key\n",
        "  w = parameters['W']\n",
        "  b = parameters['b']\n",
        "  return calculate_loss(w, b, ingredient_features, customer_likes) # Reuse the previous loss logic\n",
        "\n",
        "# Get gradient function for the dictionary input\n",
        "grad_loss_dict_fn = jax.grad(calculate_loss_dict)\n",
        "\n",
        "# Calculate gradient - result is a dictionary with the same structure\n",
        "param_gradients = grad_loss_dict_fn(params, ingredient_features, customer_likes)\n",
        "\n",
        "print(f\"\\n--- Gradient for Dictionary Parameters ---\")\n",
        "print(f\"Gradient dictionary:\\n{param_gradients}\")\n",
        "\n",
        "# Check the values match the previous individual gradients\n",
        "assert jnp.allclose(param_gradients['W'], W_gradient)\n",
        "assert jnp.allclose(param_gradients['b'], b_gradient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5OM75RS7sdx",
        "outputId": "613b69d4-28a5-4bf2-b2c7-102e49f1e49e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Gradient for Dictionary Parameters ---\n",
            "Gradient dictionary:\n",
            "{'W': Array([-0.05690404,  0.03078762, -0.52113414], dtype=float32), 'b': Array(0.45482203, dtype=float32)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Getting the Cake and the Adjustment Plan Together (`jax.value_and_grad`)\n",
        "\n",
        "Often, when optimizing, we need both the current value of our objective (e.g., the `loss` or `deliciousness_score`) *and* the gradient (how to improve it). Calculating them separately can be inefficient as they share computation. `jax.value_and_grad` does both in one go."
      ],
      "metadata": {
        "id": "RxIQG5UF9gq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a function that returns both the loss value and the gradients w.r.t. (W, b)\n",
        "value_grad_loss_fn = jax.value_and_grad(calculate_loss, argnums=(0, 1))\n",
        "\n",
        "# Call the function\n",
        "loss_val, (W_grad_vg, b_grad_vg) = value_grad_loss_fn(W, b, ingredient_features, customer_likes)\n",
        "\n",
        "print(f\"\\n--- Using value_and_grad ---\")\n",
        "print(f\"Calculated Loss: {loss_val}\")\n",
        "# Verify loss value\n",
        "assert jnp.allclose(loss_val, calculate_loss(W, b, ingredient_features, customer_likes))\n",
        "print(f\"Gradient w.r.t. W: {W_grad_vg}\")\n",
        "print(f\"Gradient w.r.t. b: {b_grad_vg}\")\n",
        "# Verify gradients\n",
        "assert jnp.allclose(W_grad_vg, W_gradient)\n",
        "assert jnp.allclose(b_grad_vg, b_gradient)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gCFQsqQ9Rjw",
        "outputId": "5674fb48-610e-48ba-e4b0-349b93a2d4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Using value_and_grad ---\n",
            "Calculated Loss: 2.247727394104004\n",
            "Gradient w.r.t. W: [-0.05690404  0.03078762 -0.52113414]\n",
            "Gradient w.r.t. b: 0.45482203364372253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2: Advanced Baking - Juggling Multiple Properties & Complex Recipes\n",
        "\n",
        "Now that we've mastered the basics of getting gradients for a single score, let's tackle more complex baking challenges."
      ],
      "metadata": {
        "id": "9ZHaZNsoAizk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Cake Properties (Jacobians: `jacfwd` & `jacrev`)\n",
        "\n",
        "Our cake has multiple important properties we care about *simultaneously*, maybe `sweetness`, `fluffiness`, and `cost`. We need the **Jacobian** matrix to see how *each* ingredient affects *each* of these properties."
      ],
      "metadata": {
        "id": "RuephxBuAvHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define ingredients as a dictionary for clarity\n",
        "initial_ingredients = {'sugar': 1.5, 'flour': 2.0, 'butter': 1.0, 'eggs': 3.0}\n",
        "ingredient_vec = jnp.array([initial_ingredients[k] for k in sorted(initial_ingredients.keys())])\n",
        "\n",
        "def bake_cake_properties(ingredients_vec):\n",
        "    \"\"\"Calculates multiple cake properties from ingredients vector.\"\"\"\n",
        "    # Fictional relationships - order matters for Jacobian\n",
        "    sugar, flour, butter, eggs = ingredients_vec[3], ingredients_vec[2], ingredients_vec[0], ingredients_vec[1]\n",
        "\n",
        "    sweetness = 10 * jnp.log1p(sugar) + 0.1 * butter - 0.2*flour\n",
        "    fluffiness = 5 * (eggs / 3.0) - 0.5 * (flour - 2.0)**2 + 0.2*butter\n",
        "    # Cost might depend non-linearly (e.g., bulk discounts implicit)\n",
        "    cost = (sugar**1.1 + flour**0.9 + butter**1.2 + eggs**1.0) * 0.5\n",
        "\n",
        "    return jnp.array([sweetness, fluffiness, cost]) # Return a vector of properties\n",
        "\n",
        "# Calculate the properties for our initial ingredients\n",
        "properties = bake_cake_properties(ingredient_vec)\n",
        "print(f\"\\n--- Jacobians: Multiple Properties ---\")\n",
        "print(f\"Initial Cake Properties (Sweetness, Fluffiness, Cost): {properties}\")\n",
        "\n",
        "# --- Calculate Jacobian using jacfwd (Forward Mode) ---\n",
        "# Efficient if num_ingredients < num_properties (\"tall\")\n",
        "# jacfwd pushes tangents forward for each input dimension\n",
        "jacobian_fwd_fn = jax.jacfwd(bake_cake_properties)\n",
        "J_fwd = jacobian_fwd_fn(ingredient_vec)\n",
        "print(\"\\nJacobian (Forward Mode - jacfwd):\")\n",
        "print(f\"Shape (num_properties, num_ingredients): {J_fwd.shape}\") # (3, 4)\n",
        "# Row i = sensitivities of property i; Col j = impact of ingredient j\n",
        "# Ingredient order: butter, eggs, flour, sugar\n",
        "print(J_fwd)\n",
        "\n",
        "# --- Calculate Jacobian using jacrev (Reverse Mode) ---\n",
        "# Efficient if num_ingredients > num_properties (\"wide\")\n",
        "# jacrev pulls cotangents back for each output dimension\n",
        "jacobian_rev_fn = jax.jacrev(bake_cake_properties)\n",
        "J_rev = jacobian_rev_fn(ingredient_vec)\n",
        "print(\"\\nJacobian (Reverse Mode - jacrev):\")\n",
        "print(f\"Shape (num_properties, num_ingredients): {J_rev.shape}\") # (3, 4)\n",
        "# Row i = sensitivities of property i; Col j = impact of ingredient j\n",
        "# Ingredient order: butter, eggs, flour, sugar\n",
        "print(J_rev)\n",
        "\n",
        "# They compute the same value\n",
        "assert jnp.allclose(J_fwd, J_rev)\n",
        "print(\"\\nForward and Reverse Jacobians match.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6rMMJWW_6_C",
        "outputId": "e0eb00db-8e4e-4ba8-a9d5-f740d35c2c58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Jacobians: Multiple Properties ---\n",
            "Initial Cake Properties (Sweetness, Fluffiness, Cost): [8.862908 5.2      3.714068]\n",
            "\n",
            "Jacobian (Forward Mode - jacfwd):\n",
            "Shape (num_properties, num_ingredients): (3, 4)\n",
            "[[ 0.1         0.         -0.2         4.        ]\n",
            " [ 0.2         1.6666667   0.          0.        ]\n",
            " [ 0.6         0.5         0.41986483  0.5727589 ]]\n",
            "\n",
            "Jacobian (Reverse Mode - jacrev):\n",
            "Shape (num_properties, num_ingredients): (3, 4)\n",
            "[[ 0.1         0.         -0.2         4.        ]\n",
            " [ 0.2         1.6666666   0.          0.        ]\n",
            " [ 0.6         0.5         0.41986483  0.5727589 ]]\n",
            "\n",
            "Forward and Reverse Jacobians match.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Jacobian `J[i, j]` tells us how property `i` (e.g., fluffiness) changes per unit change in ingredient `j` (e.g., eggs)."
      ],
      "metadata": {
        "id": "9F6HR7mfCQ4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Engine Room: JVPs and VJPs (`jax.jvp`, `jax.vjp`)\n",
        "\n",
        "`jacfwd` and `jacrev` are built on fundamental operations: Jacobian-Vector Products (JVP) for forward mode, and Vector-Jacobian Products (VJP) for reverse mode."
      ],
      "metadata": {
        "id": "cWYV_1x4ChUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **JVP** (`jax.jvp`): Answers: \"If I change the ingredients by this specific vector (the `tangents`), how will the output properties change?\" It computes `J @ v."
      ],
      "metadata": {
        "id": "x0LQTcqWCvb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a change in ingredients (tangent vector)\n",
        "# Increase sugar (+0.1), decrease flour (-0.05), keep others same\n",
        "# Order: butter, eggs, flour, sugar\n",
        "ingredient_changes = jnp.array([0.0, 0.0, -0.05, 0.1])\n",
        "\n",
        "# Compute JVP: how do properties change given ingredient_changes?\n",
        "# jvp returns (primal_output, tangent_output)\n",
        "primals_out, tangents_out = jax.jvp(bake_cake_properties, # function\n",
        "                                 (ingredient_vec,),    # where to evaluate (primals)\n",
        "                                 (ingredient_changes,)) # how inputs change (tangents)\n",
        "\n",
        "print(f\"\\n--- JVP Example ---\")\n",
        "print(f\"Original properties: {primals_out}\") # Same as 'properties' calculated before\n",
        "print(f\"Change in properties (J @ v): {tangents_out}\") # [dSweetness, dFluffiness, dCost]\n",
        "\n",
        "# Verify: Manually compute J @ v using the previously computed Jacobian\n",
        "manual_jvp = J_fwd @ ingredient_changes\n",
        "print(f\"Manual J @ v check: {manual_jvp}\")\n",
        "assert jnp.allclose(tangents_out, manual_jvp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KLJWsgBCKY9",
        "outputId": "d05612e9-2963-4255-91da-ea3f5431d098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- JVP Example ---\n",
            "Original properties: [8.862908 5.2      3.714068]\n",
            "Change in properties (J @ v): [0.40999997 0.         0.03628265]\n",
            "Manual J @ v check: [0.41       0.         0.03628265]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **VJP** (`jax.vjp`): Answers: \"If I want the properties to change by a specific vector (the `cotangent` vector `v`), what change in ingredients would cause this?\" It computes `v^T @ J`. `vjp` returns the primal output and a *function* (`vjp_fun`) to compute the backward pass.\n",
        "> **VJPs are the core mechanism behind** `jax.grad` **and** `jacrev`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2BQkwKzzDxn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define desired changes in properties (cotangent vector 'v')\n",
        "# We want: +0.2 sweetness, -0.1 fluffiness, +0 cost\n",
        "desired_property_changes = jnp.array([0.2, -0.1, 0.0])\n",
        "\n",
        "# Compute VJP:\n",
        "# 1. Forward pass to get output and vjp_fun\n",
        "primals_out_vjp, vjp_fun = jax.vjp(bake_cake_properties, ingredient_vec)\n",
        "\n",
        "# 2. Backward pass using vjp_fun\n",
        "# vjp_fun takes the cotangent vector (desired output changes)\n",
        "# and returns a tuple of gradient-like values for each primal input\n",
        "ingredient_sensitivities = vjp_fun(desired_property_changes) # Computes v^T @ J\n",
        "\n",
        "print(f\"\\n--- VJP Example ---\")\n",
        "print(f\"Original properties: {primals_out_vjp}\")\n",
        "# The result has the same structure as the input (a single vector here)\n",
        "print(f\"Ingredient sensitivities for desired change (v^T @ J): {ingredient_sensitivities[0]}\")\n",
        "\n",
        "\n",
        "# Verify: Manually compute v^T @ J\n",
        "manual_vjp = desired_property_changes @ J_fwd # Note: v is a row vector here\n",
        "print(f\"Manual v^T @ J check: {manual_vjp}\")\n",
        "assert jnp.allclose(ingredient_sensitivities[0], manual_vjp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ4ljscBDbln",
        "outputId": "36fc1d6f-03e7-4ff5-c94b-932078cac69c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- VJP Example ---\n",
            "Original properties: [8.862908 5.2      3.714068]\n",
            "Ingredient sensitivities for desired change (v^T @ J): [ 0.         -0.16666667 -0.04        0.8       ]\n",
            "Manual v^T @ J check: [ 8.1956386e-10 -1.6666667e-01 -4.0000003e-02  8.0000001e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding Curvature (Hessians)\n",
        "\n",
        "Let's define an `overall_deliciousness_score` (scalar output) and find its Hessian to understand the curvature of our deliciousness landscape."
      ],
      "metadata": {
        "id": "i6Rl9Bf2TZUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def deliciousness_score(ingredient_vec):\n",
        "    \"\"\"Combines properties into a single score.\"\"\"\n",
        "    sweetness, fluffiness, cost = bake_cake_properties(ingredient_vec)\n",
        "    # Example: reward sweetness and fluffiness, penalize cost\n",
        "    # Penalize deviation from ideal sweetness (e.g., 10) and fluffiness (e.g., 5)\n",
        "    score = -(sweetness - 10.0)**2 - (fluffiness - 5.0)**2 - 0.5 * cost\n",
        "    return score\n",
        "\n",
        "# jax.hessian(f) uses jacfwd(jacrev(f)) because forward-over-reverse is typically the most efficient.\n",
        "# It can also be written as jacrev(jacfwd(f)) or any other composition of these two.\n",
        "hessian_score_fn = jax.hessian(deliciousness_score)\n",
        "H_score = hessian_score_fn(ingredient_vec)\n",
        "\n",
        "print(f\"\\n--- Hessian of Deliciousness Score ---\")\n",
        "print(f\"Shape (num_ingredients, num_ingredients): {H_score.shape}\") # (4, 4)\n",
        "print(\"Hessian Matrix:\")\n",
        "print(H_score)\n",
        "# The Hessian tells us how the gradient of the score changes.\n",
        "# Negative diagonal elements might suggest we are near a local maximum.\n",
        "# Off-diagonal elements show how changing one ingredient affects the gradient w.r.t. another."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO4Pygh0EnFt",
        "outputId": "5faf9a4e-9e80-4ea2-8a8a-881442cbd0d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Hessian of Deliciousness Score ---\n",
            "Shape (num_ingredients, num_ingredients): (4, 4)\n",
            "Hessian Matrix:\n",
            "[[ -0.16000001  -0.6666667    0.04        -0.8       ]\n",
            " [ -0.6666667   -5.555556     0.           0.        ]\n",
            " [  0.04         0.           0.33049622   1.6       ]\n",
            " [ -0.8          0.           1.6        -35.657787  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Efficient Curvature Check (Hessian-Vector Products - HVP)\n",
        "\n",
        "For recipes with many ingredients, the full Hessian is too big. We can check curvature in one direction `v` using HVP: `H @ v`."
      ],
      "metadata": {
        "id": "yAH4g3G_Uk3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hvp(f, primals, tangents):\n",
        "  return jax.jvp(jax.jacrev(f), primals, tangents)[1]\n",
        "\n",
        "# Direction of ingredient change we want to analyze\n",
        "ingredient_direction = jnp.array([0.1, -0.2, 0.05, 0.1]) # Change butter, eggs, flour, sugar\n",
        "\n",
        "# Calculate HVP for the deliciousness score\n",
        "hvp_score_result = hvp(deliciousness_score, (ingredient_vec,), (ingredient_direction,))\n",
        "\n",
        "print(f\"\\n--- HVP of Deliciousness Score ---\")\n",
        "print(f\"HVP result in direction {ingredient_direction}:\")\n",
        "print(hvp_score_result)\n",
        "\n",
        "# Verify manually\n",
        "manual_hvp_score = H_score @ ingredient_direction\n",
        "print(f\"Manual HVP check: {manual_hvp_score}\")\n",
        "assert jnp.allclose(hvp_score_result, manual_hvp_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsFbLubkUCMB",
        "outputId": "27734eed-f1d5-429d-bb98-7309d8947b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- HVP of Deliciousness Score ---\n",
            "HVP result in direction [ 0.1  -0.2   0.05  0.1 ]:\n",
            "[ 0.03933334  1.0444444   0.18052481 -3.5657785 ]\n",
            "Manual HVP check: [ 0.03933333  1.0444446   0.18052481 -3.5657787 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Controlling Gradient Flow (`jax.lax.stop_gradient`)\n",
        "\n",
        "In recipe refinement, we might adjust `flour` to meet an `ideal_target_fluffiness`, which itself depends on other ingredients like `butter`, `eggs`, and even `flour`. Algorithmically, when calculating the gradient for `flour`, we might want to treat the `ideal_target_fluffiness` as a *fixed goal* for that step. `jax.lax.stop_gradient` achieves this by using the target's value while blocking gradient flow back through its calculation, thus focusing the update."
      ],
      "metadata": {
        "id": "WbdaPcJmVSac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_fluffiness(ingredients_vec):\n",
        "    \"\"\"Calculates cake fluffiness.\"\"\"\n",
        "    butter, eggs, flour, sugar = ingredients_vec\n",
        "    return 5 * (eggs / 3.0) - 0.5 * (flour - 2.0)**2 + 0.2 * butter\n",
        "\n",
        "def predict_ideal_fluffiness(butter, eggs, flour):\n",
        "    \"\"\"Calculates target fluffiness, including a flour dependency.\"\"\"\n",
        "    # Target depends on wet ingredients and slightly on flour\n",
        "    return 2.0 + butter * 0.5 + eggs * 1.0 - 0.1 * flour\n",
        "\n",
        "# Loss comparing actual fluffiness to the calculated ideal target\n",
        "def fluffiness_loss(ingredients_vec):\n",
        "    \"\"\"Calculates loss with full gradient dependencies.\"\"\"\n",
        "    butter, eggs, flour, sugar = ingredients_vec\n",
        "    actual_fluff = calculate_fluffiness(ingredients_vec)\n",
        "    ideal_target = predict_ideal_fluffiness(butter, eggs, flour)\n",
        "    return (actual_fluff - ideal_target)**2\n",
        "\n",
        "# Loss where the ideal target is treated as fixed for gradient calculation\n",
        "def fluffiness_loss_stopgrad(ingredients_vec):\n",
        "    \"\"\"Calculates loss, stopping gradient flow through ideal_target.\"\"\"\n",
        "    butter, eggs, flour, sugar = ingredients_vec\n",
        "    actual_fluff = calculate_fluffiness(ingredients_vec)\n",
        "    ideal_target = predict_ideal_fluffiness(butter, eggs, flour)\n",
        "    # Stop gradient flow back through ideal_target calculation\n",
        "    loss = (actual_fluff - jax.lax.stop_gradient(ideal_target))**2\n",
        "    return loss\n",
        "\n",
        "# --- Calculate and Compare Gradients ---\n",
        "print(f\"\\n--- stop_gradient Example: Focused Adjustment ---\")\n",
        "\n",
        "# Calculate gradient without stop_gradient\n",
        "grad_normal_fn = jax.grad(fluffiness_loss)\n",
        "grad_normal = grad_normal_fn(ingredient_vec)\n",
        "print(f\"Gradient WITHOUT stop_gradient:\\n{grad_normal}\")\n",
        "print(f\"  Gradient for Flour (idx 2) w/o stop_gradient: {grad_normal[2]:.4f}\")\n",
        "\n",
        "# Calculate gradient with stop_gradient\n",
        "grad_stopgrad_fn = jax.grad(fluffiness_loss_stopgrad)\n",
        "grad_stop = grad_stopgrad_fn(ingredient_vec)\n",
        "print(f\"\\nGradient WITH stop_gradient on target:\\n{grad_stop}\")\n",
        "print(f\"  Gradient for Flour (idx 2) w/  stop_gradient: {grad_stop[2]:.4f}\")\n",
        "\n",
        "# The gradients differ because the stop_gradient version ignores dT/dFlour (-0.1 here)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brM8p9qXVOOc",
        "outputId": "51e6134f-cb9b-4daf-96f9-decc778a0da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- stop_gradient Example: Focused Adjustment ---\n",
            "Gradient WITHOUT stop_gradient:\n",
            "[ 0.06000023 -0.13333383 -0.02000008  0.        ]\n",
            "  Gradient for Flour (idx 2) w/o stop_gradient: -0.0200\n",
            "\n",
            "Gradient WITH stop_gradient on target:\n",
            "[-0.04000015 -0.3333346   0.          0.        ]\n",
            "  Gradient for Flour (idx 2) w/  stop_gradient: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baking for Many Customers (`vmap` + `grad`)\n",
        "\n",
        "We need to bake slightly different cakes for a batch of customer orders, each with a target `sweetness`. We want the gradient of `(actual_sweetness - target_sweetness)**2` for each customer, using *their* specific target."
      ],
      "metadata": {
        "id": "orFV-8LJXJWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sweetness_error(ingredients_vec, target_sweet):\n",
        "    \"\"\"Squared error for sweetness.\"\"\"\n",
        "    # bake_cake_properties returns [sweetness, fluffiness, cost]\n",
        "    actual_sweetness = bake_cake_properties(ingredients_vec)[0]\n",
        "    return (actual_sweetness - target_sweet)**2\n",
        "\n",
        "# Batch of target sweetness levels for different customers\n",
        "target_sweetness_batch = jnp.array([9.5, 10.0, 10.5, 9.8])\n",
        "\n",
        "# Gradient function for one target\n",
        "grad_sweet_error_fn = jax.grad(sweetness_error, argnums=0) # Grad w.r.t ingredients\n",
        "\n",
        "# Map the gradient function over the batch of targets\n",
        "# Keep ingredients fixed (None), map over target_sweet (axis 0)\n",
        "per_customer_grad_fn = jax.vmap(grad_sweet_error_fn, in_axes=(None, 0))\n",
        "\n",
        "# Calculate all gradients\n",
        "per_customer_ingredient_grads = per_customer_grad_fn(ingredient_vec, target_sweetness_batch)\n",
        "\n",
        "print(f\"\\n--- Per-Example Gradients (vmap + grad) ---\")\n",
        "print(f\"Shape of gradients: {per_customer_ingredient_grads.shape}\") # (batch_size, num_ingredients) -> (4, 4)\n",
        "print(\"Gradients per customer order:\")\n",
        "print(per_customer_ingredient_grads)\n",
        "# Each row tells how to adjust 'ingredient_vec' to reduce sweetness error for that specific customer."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwX1lxuCWU2v",
        "outputId": "439802c0-e9d1-4147-a09c-e8f558b21249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Per-Example Gradients (vmap + grad) ---\n",
            "Shape of gradients: (4, 4)\n",
            "Gradients per customer order:\n",
            "[[ -0.12741832   0.           0.25483665  -5.096733  ]\n",
            " [ -0.22741833   0.           0.45483667  -9.096733  ]\n",
            " [ -0.32741833   0.           0.65483665 -13.096733  ]\n",
            " [ -0.18741837   0.           0.37483674  -7.4967346 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Exotic Flavors Using Complex Numbers\n",
        "\n",
        "Here, we'll explore how JAX handles even more advanced situations using mystical complex numbers in our analysis"
      ],
      "metadata": {
        "id": "xX5PVBPhshLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complex Flavors & Vibrations: Differentiation with Complex Numbers\n",
        "\n",
        "Sometimes, simple numbers aren't enough. Maybe we're inventing truly magical cakes where flavors interact in ways best described by *complex* numbers (numbers with a 'real' part and an 'imaginary' part, like `3 + 4j`). Or perhaps we're analyzing the complex vibrations (`amplitude` + `phase`) in our mixer batter, knowing they affect the final texture. Can JAX handle derivatives involving these \"magic numbers\"? Yes!"
      ],
      "metadata": {
        "id": "RlajmVnmtIhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Nuance: Smooth Magic vs. Tricky Magic\n",
        "\n",
        "Think of complex functions like magic spells:\n",
        "\n",
        "1. **Holomorphic Spells (Smooth Magic)**: These behave nicely, like stretching and rotating things smoothly. Functions like $f(z)=z^2$ or $f(z)=sin(z)$ are like this. Their derivative is just one complex number, similar to regular calculus.\n",
        "2. **Non-Holomorphic Spells (Tricky Magic)**: These spells might involve sharp changes or depend on direction. Examples are taking just the real part $(f(z)=Re(z))$ or the complex conjugate $(f(z)=\\bar{z}=x‚àíiy)$. Their \"derivative\" is more complicated; it needs to describe how both the real and imaginary output parts change when either the real or imaginary input part changes (like a 2x2 chart, or Jacobian)."
      ],
      "metadata": {
        "id": "qxqrrUG9tfyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX's Approach: The Universal Magic Wand (JVPs & VJPs)\n",
        "\n",
        "JAX's fundamental tools, JVP and VJP, are built to handle *all* types of complex magic correctly. They work by tracking the real and imaginary parts separately.\n",
        "\n",
        "*Think like this*: A complex number $z=x+iy$ is like a point $(x,y)$ on a 2D map. A function $f(z)=u(x,y)+iv(x,y)$ takes a point on one map and gives a point on another map. JAX's JVP and VJP figure out the derivatives for this underlying $R^2‚ÜíR^2$ map.\n",
        "\n",
        "* `jvp(f, (z,), (dz,))`: If you wiggle the input `z` by a complex amount `dz`, this tells you the complex wiggle `df` in the output `f(z)`. It works for any complex function `f`.\n",
        "* `vjp(f, z)`: Gives you the output `f(z)` and a \"pullback\" function. This pullback function takes a desired complex output wiggle `g` and tells you the complex input wiggle `dz` needed to cause it. It also works for any complex function `f`."
      ],
      "metadata": {
        "id": "8yJhwaMju6gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `jax.grad` (The Easy Spellbook)\n",
        "\n",
        "The `jax.grad` function is simpler but has specific rules for complex numbers:"
      ],
      "metadata": {
        "id": "rQFr9bdt939w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Magic Input -> Real Score (**`C -> R`**)**: If our magical vibration `z` affects the real `deliciousness_score`, `jax.grad` works directly! It gives a *complex* gradient. This complex number tells you the direction on the complex z plane to move z to increase the score the fastest."
      ],
      "metadata": {
        "id": "FeQv9WDjwHac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_from_vibration(z):\n",
        "  \"\"\"Real score based on complex vibration z = x + iy.\"\"\"\n",
        "  # Score increases with real part, decreases with distance from origin\n",
        "  return jnp.real(z) * 3.0 - jnp.real(z * jnp.conjugate(z)) # 3x - (x^2 + y^2)\n",
        "\n",
        "z_vibration = 2.0 + 1.0j\n",
        "current_score = score_from_vibration(z_vibration)\n",
        "print(f\"\\n--- Complex Input -> Real Output ---\")\n",
        "print(f\"Vibration z = {z_vibration}, Score = {current_score:.3f}\")\n",
        "\n",
        "# Get the complex gradient\n",
        "grad_score_complex_fn = jax.grad(score_from_vibration)\n",
        "complex_grad = grad_score_complex_fn(z_vibration)\n",
        "print(f\"Complex Gradient ‚àá_z score: {complex_grad}\")\n",
        "# dScore/dx = 3 - 2x = 3 - 4.0 = -1.0. dScore/dy = 2y = 2.0. Grad = -1.0 + 2.0j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qQNgKNQwtX8",
        "outputId": "0fc0cdb8-f3ad-4644-c366-55b2dc27a2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Complex Input -> Real Output ---\n",
            "Vibration z = (2+1j), Score = 1.000\n",
            "Complex Gradient ‚àá_z score: (-1+2j)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Magic Input -> Magic Output (**`C -> C`)**:**\n",
        "\n",
        "  **Holomorphic (\"Smooth Magic\")**: You must tell `grad` it's smooth magic with `holomorphic=True`. It then calculates the correct single complex derivative."
      ],
      "metadata": {
        "id": "tduC1k3RyS3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth_magic_process(z):\n",
        "  \"\"\"A holomorphic function.\"\"\"\n",
        "  return z**2 + z\n",
        "\n",
        "# Must use holomorphic=True for C->C\n",
        "grad_smooth_magic_fn = jax.grad(smooth_magic_process, holomorphic=True)\n",
        "complex_deriv = grad_smooth_magic_fn(z_vibration)\n",
        "print(f\"\\n--- Holomorphic C -> C Example (z^2 + z) ---\")\n",
        "print(f\"Input z = {z_vibration}\")\n",
        "print(f\"Output f(z) = {smooth_magic_process(z_vibration)}\")\n",
        "print(f\"Complex Derivative df/dz: {complex_deriv}\")\n",
        "# Check: derivative is 2z + 1 = 2*(2.0+1.0j) + 1 = 4 + 2.0j + 1 = 5.0 + 2.0j\n",
        "assert jnp.allclose(complex_deriv, 2 * z_vibration + 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJPISXqlxXvJ",
        "outputId": "84ed0a4e-91c4-41e2-d0d3-17e0fea4fc25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Holomorphic C -> C Example (z^2 + z) ---\n",
            "Input z = (2+1j)\n",
            "Output f(z) = (5+5j)\n",
            "Complex Derivative df/dz: (5+2j)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Non-Holomorphic (\"Tricky Magic\")**: Using `grad(..., holomorphic=True)` doesn't give you the whole magic picture for tricky spells. We can still write `holomorphic=True` when the function isn't holomorphic (this stops JAX from giving an error just because the output is complex), but the answer we get out won't represent the *full Jacobian* [the complete derivative information].\n",
        "  \n",
        "  Instead, it'll usually be the gradient of the function where we just discard the imaginary part of the output: (i.e., the gradient of $Re(f(z))$). This complex number result can be misleading if you need to know how all parts of the magic change. JAX will usually raise an error if you try `jax.grad` on a `C->C` spell without `holomorphic=True`. For the full picture (how real/imaginary inputs affect real/imaginary outputs), use `jax.jacfwd` or `jax.jacrev` on the function viewed as mapping $R^2‚ÜíR^2$."
      ],
      "metadata": {
        "id": "P0KvHJJt3yac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tricky_magic_process(z):\n",
        "  \"\"\"Non-holomorphic: complex conjugate.\"\"\"\n",
        "  return jnp.conjugate(z) # Example: conj(x+iy) = x-iy\n",
        "\n",
        "# Using grad with holomorphic=True gives a specific, potentially misleading, result\n",
        "grad_tricky_magic_fn = jax.grad(tricky_magic_process, holomorphic=True)\n",
        "misleading_result = grad_tricky_magic_fn(z_vibration)\n",
        "print(f\"\\n--- Non-Holomorphic C -> C Example (conj(z)) ---\")\n",
        "print(f\"Using grad(..., holomorphic=True) on conj(z): {misleading_result}\")\n",
        "# For conj(z)=x-iy, u=x, v=-y. grad(..., holo=True) gives du/dx + i*du/dy = 1 + 0j = 1.0\n",
        "\n",
        "# Correct way: Use Jacobian on R^2 -> R^2 view\n",
        "def tricky_magic_real_view(real_vec):\n",
        "     z = real_vec[0] + 1j * real_vec[1]\n",
        "     out_complex = tricky_magic_process(z)\n",
        "     # Return [real_output, imaginary_output]\n",
        "     return jnp.array([jnp.real(out_complex), jnp.imag(out_complex)])\n",
        "\n",
        "jacobian_fn = jax.jacrev(tricky_magic_real_view)\n",
        "# Use the real/imag parts of z_vibration = 1.2 + 0.5j\n",
        "real_jacobian_info = jacobian_fn(jnp.array([1.2, 0.5]))\n",
        "print(f\"\\nFull Jacobian via R^2 view for conj(z):\")\n",
        "print(real_jacobian_info)\n",
        "# Expected Jacobian for f(x,y) = [x, -y] w.r.t. [x, y] is [[1, 0], [0, -1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-ut6obdzGha",
        "outputId": "4251f5dd-73b6-43aa-e3c5-ee4a36aeeafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Non-Holomorphic C -> C Example (conj(z)) ---\n",
            "Using grad(..., holomorphic=True) on conj(z): (1-0j)\n",
            "\n",
            "Full Jacobian via R^2 view for conj(z):\n",
            "[[ 1.  0.]\n",
            " [ 0. -1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Real Recipe, Magic Inside (**`R -> R`**)**: If your normal recipe (real inputs/outputs) uses complex math internally (like special mixing algorithms using FFTs), `jax.grad` works just fine and gives the correct real gradients. JAX handles the complex steps internally via JVPs/VJPs."
      ],
      "metadata": {
        "id": "BjaHWRek7eD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_flavor_stability(ingredient_potency_x):\n",
        "  \"\"\"\n",
        "  Calculates a real 'stability' score based on potency 'x'.\n",
        "  Uses complex math internally. R -> C -> R function.\n",
        "  \"\"\"\n",
        "  # Create a complex number based on the real input potency 'x'\n",
        "  z_activation = 1.0 + ingredient_potency_x * 1j  # e.g., z = 1 + ix\n",
        "\n",
        "  # Perform some calculation involving complex numbers\n",
        "  # Example: compute squared magnitude |z|^2 = (1+ix)(1-ix) = 1 + x^2\n",
        "  stability_score = z_activation * jnp.conjugate(z_activation)\n",
        "  return jnp.real(stability_score)\n",
        "\n",
        "ingredient_potency = 3.0\n",
        "\n",
        "print(f\"Input Ingredient Potency x = {ingredient_potency}\")\n",
        "# Calculate the score directly\n",
        "stability = calculate_flavor_stability(ingredient_potency)\n",
        "# Expected: 1 + 3.0^2 = 10.0\n",
        "print(f\"Output Flavor Stability Score = {stability:.4f}\")\n",
        "\n",
        "# Even though we created complex numbers inside, the function maps R -> R.\n",
        "# jax.grad works seamlessly.\n",
        "grad_stability_fn = jax.grad(calculate_flavor_stability)\n",
        "stability_gradient = grad_stability_fn(ingredient_potency)\n",
        "\n",
        "print(f\"\\nGradient d(Score)/dx at x={ingredient_potency}: {stability_gradient:.4f}\")\n",
        "# Check: Derivative of 1 + x^2 is 2x. For x=3.0, derivative is 6.0\n",
        "assert jnp.allclose(stability_gradient, 2 * ingredient_potency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K9Pg7A85rPp",
        "outputId": "fefb3da7-677c-4312-99a7-215ebfb222e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Ingredient Potency x = 3.0\n",
            "Output Flavor Stability Score = 10.0000\n",
            "\n",
            "Gradient d(Score)/dx at x=3.0: 6.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Secret Family Recipes & Advanced Techniques!\n",
        "\n",
        "We've seen how our JAX assistant can handle tricky calculations, multiple outputs, and even complex numbers. But sometimes, the standard way JAX figures out derivatives isn't quite right, or we have a special technique we want to use. Maybe JAX's calculation causes a kitchen disaster (`NaN!`), or we have a secret family rule for how much an ingredient should affect the taste.\n",
        "\n",
        "Fear not! JAX lets us *teach* it our own **Custom Differentiation Rules**. Think of it like passing down a secret family recipe or technique that JAX wouldn't know otherwise. We'll focus on the main tools for this: `jax.custom_jvp` and `jax.custom_vjp`."
      ],
      "metadata": {
        "id": "8Gcu2Ai6Bjjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Use Custom Rules? (Special Baking Scenarios)\n",
        "\n",
        "Why would we override JAX's smarts?\n",
        "\n",
        "* **Avoiding Kitchen Disasters (Numerical Stability)**: Sometimes, a standard calculation (like for yeast activity at high temperatures) might involve numbers so big or small that JAX's automatic derivative gives `NaN` or `inf`. We can provide a mathematically equivalent, but more stable, formula just for the derivative.\n",
        "\n",
        "* **Enforcing Kitchen Rules (Gradient Modification)**: We might have a strict rule: \"Adding more butter should never increase the 'lightness cost' gradient by more than X units\". We can enforce this by directly modifying the gradient during the calculation.\n",
        "\n",
        "* **Handling Secret Processes (Implicit Functions/Loops)**: Our unique sourdough starter might mature in a complex way described by a loop (`while_loop`) that stops when ready. Differentiating through thousands of loop steps is often impossible or inefficient for grad. We can use math shortcuts (like the Implicit Function Theorem) to figure out the derivative of the final state and teach that shortcut to JAX.\n"
      ],
      "metadata": {
        "id": "7iSIJ-RzCkeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Teaching JAX: `jax.custom_jvp` (Forward Changes)\n",
        "\n",
        "Use `custom_jvp` when you want to define how the output changes (`output_tangent`) given a specific change in the inputs (`tangents`). JAX can often figure out the `grad` rule from this too!\n",
        "\n",
        "Imagine yeast activity explodes at high temperatures, causing `exp(temp)` to give `inf` in gradients."
      ],
      "metadata": {
        "id": "5u-JEdlIDfy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.custom_jvp\n",
        "def yeast_activity(temp):\n",
        "  # Activity increases sharply, potentially unstable derivative\n",
        "  return jnp.log(1. + jnp.exp(temp))\n",
        "\n",
        "# Custom JVP rule\n",
        "@yeast_activity.defjvp\n",
        "def yeast_activity_jvp(primals, tangents):\n",
        "  temp, = primals\n",
        "  temp_dot, = tangents # How temperature is changing\n",
        "  activity = yeast_activity(temp) # Original function value\n",
        "\n",
        "  # Standard derivative: (1 / (1 + jnp.exp(temp))) * jnp.exp(temp) * temp_dot\n",
        "  # for high temp effectively turns into 0. * jnp.inf * temp_dot\n",
        "  # Stable alternative: (1 - 1/(1 + jnp.exp(temp))) * temp_dot\n",
        "  activity_dot = (1 - 1/(1 + jnp.exp(temp))) * temp_dot # More stable calculation\n",
        "\n",
        "  return activity, activity_dot\n",
        "\n",
        "# --- Test ---\n",
        "high_temp = 500.0 # A high temperature where exp might be huge\n",
        "\n",
        "print(f\"\\n--- Custom JVP (Yeast Stability) ---\")\n",
        "print(f\"Yeast activity at {high_temp}: {yeast_activity(high_temp)}\")\n",
        "# grad will use the custom rule\n",
        "print(f\"Gradient of activity at {high_temp}: {jax.grad(yeast_activity)(high_temp)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1x9uS5eB6B7",
        "outputId": "6489e70c-9b44-4e2e-81ee-a7319d1241d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Custom JVP (Yeast Stability) ---\n",
            "Yeast activity at 500.0: inf\n",
            "Gradient of activity at 500.0: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Teaching JAX: `jax.custom_vjp` (Backward Sensitivity)\n",
        "\n",
        "Use `custom_vjp` when you need direct control over the backward pass used by `grad`. This is needed for gradient clipping or handling loops/implicit functions. *Note*: This only defines the rule for grad/reverse-mode; forward-mode (`jvp`) won't work.\n",
        "\n",
        "Let's enforce a rule: the gradient component for `butter` (index 0) cannot have a magnitude larger than `butter_range`.\n",
        "\n",
        "1. Decorate `f` with `@custom_vjp`.\n",
        "2. Define `f_fwd(...)`: Takes same inputs as `f`, returns (`original_output`, `residuals`). `residuals` are your \"notes\" to pass to the backward step.\n",
        "3. Define `f_bwd(residuals, output_grad)`: Takes the notes and the incoming gradient (`output_grad` - how later steps want the output to change), returns a tuple of gradients for each input of `f`.\n",
        "Link them: `f.defvjp(f_fwd, f_bwd)`"
      ],
      "metadata": {
        "id": "BYYwL83ZIMhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.custom_vjp\n",
        "def pass_through_with_butter_limit(ingredient_vec, butter_range):\n",
        "  \"\"\"Passes ingredients through, but limits butter grad.\"\"\"\n",
        "  return ingredient_vec\n",
        "\n",
        "# Forward: Output is input, no notes needed\n",
        "def butter_limit_fwd(ingredient_vec, butter_range):\n",
        "  return ingredient_vec, butter_range # output, residuals\n",
        "\n",
        "# Backward: Apply clipping to butter component (index 0) of incoming grad\n",
        "def butter_limit_bwd(residuals, grad_in):\n",
        "  butter_grad = grad_in[0]\n",
        "  clipped_butter_grad = jnp.clip(butter_grad, -residuals, residuals)\n",
        "  # Create the output gradient tuple, replacing the butter component\n",
        "  # use None to indicate zero cotangents for x\n",
        "  return (grad_in.at[0].set(clipped_butter_grad), None)\n",
        "\n",
        "pass_through_with_butter_limit.defvjp(butter_limit_fwd, butter_limit_bwd)\n",
        "\n",
        "# --- Test ---\n",
        "def score_with_butter_limit(ingredient_vec):\n",
        "   # Apply the custom rule function before the score\n",
        "   adjusted_for_grad = pass_through_with_butter_limit(ingredient_vec, 0.1)\n",
        "   return deliciousness_score(adjusted_for_grad)\n",
        "\n",
        "# Calculate gradients\n",
        "grad_original_fn = jax.grad(deliciousness_score)\n",
        "grad_clipped_fn = jax.grad(score_with_butter_limit)\n",
        "original_grad = grad_original_fn(ingredient_vec)\n",
        "clipped_grad = grad_clipped_fn(ingredient_vec)\n",
        "\n",
        "print(f\"\\n--- Custom VJP (Butter Gradient Clipping) ---\")\n",
        "print(f\"Original gradient:       {original_grad}\")\n",
        "print(f\"Gradient w/ butter clip: {clipped_grad}\")\n",
        "print(f\"(Note butter grad [index 0] is clipped to +/- 0.1)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-pD6edsFeV_",
        "outputId": "0badf98d-c111-45d6-980a-166f5100873e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Custom VJP (Butter Gradient Clipping) ---\n",
            "Original gradient:       [-0.1525816  -0.91666603 -0.66476905  8.810353  ]\n",
            "Gradient w/ butter clip: [-0.1        -0.91666603 -0.66476905  8.810353  ]\n",
            "(Note butter grad [index 0] is clipped to +/- 0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling a Secret Process (Dough Maturation)\n",
        "\n",
        "Imagine our sourdough's final \"maturity\" level depends on a base \"boost\" (`b` from sugar/yeast) and how much maturity is retained (`a` < 1, from gluten strength) each hour in a loop: `maturity = a * maturity + b`. Finding the final stable maturity `maturity_star` involves this loop. We use `custom_vjp` to find `d(maturity_star)/da` and `d(maturity_star)/db` using a math shortcut, as `grad` might struggle with the loop."
      ],
      "metadata": {
        "id": "VuKNEQstQZVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.custom_vjp\n",
        "def find_final_dough_maturity(params, initial_maturity, tol=1e-6):\n",
        "  \"\"\"Finds equilibrium maturity x* where x* = a*x* + b via iteration.\"\"\"\n",
        "  a_retention, b_boost = params\n",
        "  update = lambda x: a_retention * x + b_boost\n",
        "  # Simple fixed-point loop\n",
        "  state = (initial_maturity, update(initial_maturity))\n",
        "  maturity_star = jax.lax.while_loop(lambda s: jnp.abs(s[0]-s[1]) > tol,\n",
        "                             lambda s: (s[1], update(s[1])),\n",
        "                             state)[1]\n",
        "  return maturity_star\n",
        "\n",
        "# Forward pass: Run iteration, save needed values.\n",
        "def maturity_fwd(params, initial_maturity, tol=1e-6):\n",
        "  maturity_star = find_final_dough_maturity(params, initial_maturity, tol)\n",
        "  return maturity_star, (params, maturity_star) # Save params(a,b) and result x*\n",
        "\n",
        "# Backward pass: Apply analytical shortcut for x = ax + b derivative.\n",
        "def maturity_bwd(residuals, maturity_star_bar):\n",
        "  params, maturity_star = residuals\n",
        "  a_retention, b_boost = params\n",
        "  # Analytical shortcut based on implicit function theorem:\n",
        "  # w = (dL/dx) / (1 - a); dL/da = w*x; dL/db = w\n",
        "  w = maturity_star_bar / (1.0 - a_retention + 1e-8) # Add epsilon for stability near a=1\n",
        "  a_grad = w * maturity_star\n",
        "  b_grad = w\n",
        "  # Return gradients for inputs: ((a_grad, b_grad), guess_grad)\n",
        "  return ((a_grad, b_grad), None, None) # No grad for initial guess and tol\n",
        "\n",
        "# Link the forward and backward rules\n",
        "find_final_dough_maturity.defvjp(maturity_fwd, maturity_bwd)\n",
        "\n",
        "current_params = (0.75, 10.0) # Equilibrium = 10 / (1-0.75) = 40\n",
        "initial_guess = 0.0\n",
        "\n",
        "print(f\"\\n--- Custom VJP (Minimal Loop Example) ---\")\n",
        "# Calculate gradient of final maturity w.r.t. params (a, b)\n",
        "# JAX uses our custom VJP rules, not the loop.\n",
        "param_sensitivity = jax.grad(find_final_dough_maturity)(current_params, initial_guess)\n",
        "\n",
        "# Expected analytical: dM*/da=b/(1-a)^2=10/(0.25^2)=160; dM*/db=1/(1-a)=1/0.25=4\n",
        "print(f\"Gradient d(Maturity)/d(params={current_params}): ({param_sensitivity[0]:.2f}, {param_sensitivity[1]:.2f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed4E2RySLVyT",
        "outputId": "8d84167b-a4fc-45bf-992d-6aee9304f792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Custom VJP (Minimal Loop Example) ---\n",
            "Gradient d(Maturity)/d(params=(0.75, 10.0)): (160.00, 4.00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion: Bake Like a Pro with JAX! üç∞\n",
        "\n",
        "Right then, Baker! You've officially graduated from JAX Autodiff Culinary Academy üéì. You didn't just bake a cake, you *differentiated* the heck out of it! ü§Ø\n",
        "\n",
        "You started with simple `jax.grad` (the \"more sugar = more happy?\" calculation ü§î), but quickly graduated to the wild stuff: Jacobians mapping sugar-to-sadness-and-fluffiness, Hessians checking if your deliciousness peak is pointy or flat, and HVPs for a quick curvature peek without melting your machine üî•.\n",
        "\n",
        "You learned the kitchen commands: `jax.lax.stop_gradient` (telling JAX 'just aim for *this* fluffiness target, don't ask where it came from!'), `vmap` (getting picky gradients for *all* your demanding customers at once), and even wrangled complex numbers - because apparently, imaginary flavor dimensions are a thing now, and JAX is cool with it.\n",
        "\n",
        "Best of all? When JAX just didn't get your ancient sourdough loop-de-loop or that ingredient prone to numerical kitchen fires, you showed it who's boss üòé with `jax.custom_jvp` and `jax.custom_vjp` - your Secret Family Recipe scrolls üìú.\n",
        "\n",
        "You're now dangerously equipped to optimize practically anything. Go forth and `grad`! (Maybe bake a real cake now üéÇ, you've earned it)."
      ],
      "metadata": {
        "id": "rRg0PnB8X7Vb"
      }
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "er-HdHiJ029r",
        "RuephxBuAvHQ",
        "cWYV_1x4ChUp"
      ],
      "authorship_tag": "ABX9TyNc4aOgAbppfxl5y9mZ0Mpg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}